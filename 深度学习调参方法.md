# 深度学习调参方法

**Varun Godbole†, George E. Dahl†, Justin Gilmer†, Christopher J. Shallue‡, Zachary Nado†**

† Google Research, Brain Team

‡ Harvard University

## 开始新的工程的指南

​		许多调参的决定只在工程开始时设置，当一些条件发生变化时才会再去回顾或改变。

​		我们该部分的指南基于以下假设：

1. 问题构建、数据清理等主要工作已经完成，因此，只需将时间花在模型架构和训练配置
2. 已经有流水线用来进行模型训练和评估，并且，对所感兴趣的不同模型都可以较容易地进行训练和预测工作
3. 合适的评估标准已经选择并设置。这些应该尽可能对部署环境下的测量有代表性



### 选择合适的模型架构

**总结**：当开始一个新的工程时，试着复用已经有效果的模型

1. 首先，选择一个很好构建、普遍使用的模型架构来开展工作。总是可以在后面再建立一个定制化的模型

2. 模型架构一般都具有多种超参数，这些超参数决定了模型的尺寸和其他一些细节（如，层数，层宽，激活函数的类型）

   a. 因此，选择一个架构，实际意味着选择一族不同模型（一组模型超参数的设置即为一族）

   b. 我们将在选择初始设置和提升模型性能的科学方法中研究如何选择模型超参数的问题

3. 如果可能的话，试着寻找一篇与手头工作尽可能接近的论文，并复现该模型作为开始



### 选择优化器

**总结**：以对手头问题最流行的优化器来开始

1. 对于所有类型的机器学习问题和模型架构，没有哪个优化器是最优的。甚至，比较不同优化器也是个困难的任务。

2. 我们推荐以流行的优化器，特别是当开始一个新的工程。

   a. 理想的，对相同类型的问题选择最流行的优化器

3. 应对所选择的优化器的所有超参数给予重视。

   a. 具有更多超参数的优化器可能需要更多的调节努力来寻找最佳设置
   
   b. 这特别的相对在工程开始阶段，当我们寻找其他多种超参数的最佳值（如，架构超参数）而对优化器超参数为麻烦的参数
   
   c. 也许在工程开始阶段，使用较简单的优化器（如，固定动量的SGD或固定参数的Adam）更合适，在以后再转向更一般的优化器

4. 我们喜欢公认的优化器包括（但不局限于）

   a. 动量SGD（我们喜欢Nesterov变种）

   b. Adam和NAdam，比动量SGD更一般。注意，Adam有4个可调节超参数。

### 选择batch size

**总结**：batch size控制着训练速度，不应被用于直接调节验证集的性能。通常，理想的batch size是现有硬件所支持的最大的batch size。

1. batch size是决定训练时间和计算资源消耗的关键参数

2. 增加batch size通常可以降低训练时间。这是非常有用的，因为

   a. 在固定的时间间隔内，使得超参数可以被调节更多次，潜在地导致更好的最终模型

   b. 减小开发周期的等待时间，使得新的idea能够更频繁地被测试

3. 增加batch size可能会降低、增加或不改变资源消耗

4. batch size不该被视作用于改变验证集性能的可调节超参数

   a. 只要所有超参数已经被很好地调节（特别是学习速率和正则化超参数），以及训练步数是足够的，那么，对于任意batch size都能得到相同的最终性能(Shallue et al. 2018)

   b. 可以查看为什么不应该直接调节batch size 来提升验证集的性能

#### 确定可行的batch size和估计训练吞吐量(throughput)

1. 对于给定的模型和优化器，现有硬件一般支持一定范围的batch size。这限制因素通常为内存

2. 不幸的是，不进行运行或编译完整的训练程序，是很难计算适合内存的batch size

3. 最简单的解决方法，通常是用较少步数分别训练一系列不同batch size，直到其中一个job超过了现有内存

4. 对于每个batch size，我们应该训练足够长的时间来获得对训练吞吐量的可靠估计。

   ​	训练吞吐量 =（ 每秒处理的样本数） 或 （每个step所需时间）

   ​	每个step所需时间 = （batch size）/ （训练吞吐量）

5. 当加速器还没达到饱和，如果加倍batch size，则训练吞吐量也应加倍（或至少接近加倍）。相当于，当batch size增加，每一步的时长应为恒值（至少近似恒值）

6. 如果不是这种情况，那么训练流水线会有其他瓶颈，如，IO或在计算节点间的同步。这值得在下一步进行之前先诊断和修正。

7. 如果训练吞吐量只在一定大的batch size会增加，那么，我们只考虑这种大小batch size，即使硬件支持更大的batch size

   a. 使用更大的batch size的好处是基于训练吞吐量会增加。如果没有满足这种假设，那么应修复这些瓶颈或使用更小的batch size

   b. **梯度累加**模拟比硬件支持的最大batch size，因此，不会得到任何吞吐量的好处。这一般在实际工作中应避免。

8. 这些步骤可能在每次模型或优化器发生改变时都要重复进行（如，不同的模型架构可能运行更大的batch size适合内存）

 

#### 选择batch size来使得训练时间最小化

训练时长 = （每个step时长）* （总step数）

1. 我们通常可以认为，对于所有可行的batch size，每个step的时间都是近似恒定的。当并行计算没有开销并且所有训练瓶颈都已被诊断和纠正时，这是正确的。实际上，增加batch size通常至少会产生一些开销。

2. 随着batch size的增加，达到固定性能目标所需的步数通常会减少（Shallue et al. 2018 给出了当batch size发生改变时，重调所有相关超参数的方法）

   a. 例如，加倍batch size可能会使得所需步数减半。这被称为是完美尺度化。

   b. 在达到临界batch size之前，完美尺度化对所有batch size都是成立的；超过该临界值后，得到的回报将递减

   c. 最终，增加batch size不再减少训练所需步数（但从不会增加步数）

3. 因此，最小化训练时间的batch size，通常是最大的batch size仍能减小所需的训练步数。

   a. 这个批量大小取决于数据集、模型和优化器，除了通过实验为每个新问题找到它之外，如何计算它是一个悬而未决的问题

   b. 当比较不同batch size时，需注意单个样本开销/epoch开销(当固定训练样本数时训练所有实验)和单个step的开销(固定训练步数来训练所有实验)

   ​	将批量大小与 epoch 预算进行比较只会探索完美的扩展机制，即使更大的批量大小仍可能通过减少所需的训练步骤数来提供有意义的加速

   c. 通常，硬件所支持的最大batch size会比临界batch size要小一些。因此，好的做法是（没有跑任何实验）尽可能使用最大的batch size。

4. 如果最终会增加训练时间，那么使用更大的批量大小是没有意义的



#### 选择使得资源开销最小化的batch size

1. 伴随着batch size的增加，有两种类型的资源开销

2. 如果增加批量大小的前期成本很高，那么最好推迟增加批量大小，直到项目成熟并且更容易评估成本效益权衡。 实施多主机并行训练程序可能会引入错误和细微问题，因此最好从更简单的管道开始。 （另一方面，在需要大量调整实验的过程早期，训练时间的大幅加快可能非常有益）

3. 我们将总的使用成本（可能包括多种不同的成本）称为“资源消耗”。 我们可以将资源消耗分解为以下组成部分：

   资源消耗 = （每个step的资源消耗）* （总step数量）

4. 增加batch size通常可以让我们减少总步数。 资源消耗是增加还是减少将取决于每步消耗如何变化

   a. 增加批大小可能会减少资源消耗。 例如，如果具有较大batch size的每个step都可以在与较小批量大小相同的硬件上运行（每个step的时间只增加一点点），那么每个step资源消耗的增加可能会被步数上的减少所抵消。

   b. 增加batch size可能不会改变资源消耗。例如，如果加倍batch size来使所需step减半和加倍所需GPU数量，那么总的消耗（GPU小时）不会改变。

   c. 增加batch size可能增加资源消耗。例如，如果增加batch size会需要升级硬件，每个step的消耗的增加可能会被step数量的减少而抵消。



#### 改变batch size会需要重调大部分超参数

1. 大部分超参数的最优值都是对batch size敏感的。因此，当改变batch size时，一般都需要重新调参
2. 和batch size相关性最强的超参数是优化器超参数（如，学习速率和动量）和正则化超参数，因此，需要对每个batch size单独调参。
3. 在项目开始时选择批量大小时请记住这一点。 如果您以后需要切换到不同的批量大小，则为新的批量大小重新调整所有内容可能会很困难、耗时且昂贵。



#### Batch norm如何与batch size相互影响

​	Batch norm 很复杂，一般来说，应该使用与梯度计算不同的 batch size 来计算统计数据。 有关详细讨论，请参阅batch norm部分

























