# 深度学习调参方法

**Varun Godbole†, George E. Dahl†, Justin Gilmer†, Christopher J. Shallue‡, Zachary Nado†**

† Google Research, Brain Team

‡ Harvard University

## 开始新的工程的指南

​		许多调参的决定只在工程开始时设置，当一些条件发生变化时才会再去回顾或改变。

​		我们该部分的指南基于以下假设：

		1. 问题构建、数据清理等主要工作已经完成，因此，只需将时间花在模型架构和训练配置
		1. 已经有流水线用来进行模型训练和评估，并且，对所感兴趣的不同模型都可以较容易地进行训练和预测工作
		1. 合适的评估标准已经选择并设置。这些应该尽可能对部署环境下的测量有代表性



### 选择合适的模型架构

**总结**：当开始一个新的工程时，试着复用已经有效果的模型

1. 首先，选择一个很好构建、普遍使用的模型架构来开展工作。总是可以在后面再建立一个定制化的模型

2. 模型架构一般都具有多种超参数，这些超参数决定了模型的尺寸和其他一些细节（如，层数，层宽，激活函数的类型）

   a. 因此，选择一个架构，实际意味着选择一族不同模型（一组模型超参数的设置即为一族）

   b. 我们将在选择初始设置和提升模型性能的科学方法中研究如何选择模型超参数的问题

3. 如果可能的话，试着寻找一篇与手头工作尽可能接近的论文，并复现该模型作为开始



### 选择优化器

**总结**：以对手头问题最流行的优化器来开始

1. 对于所有类型的机器学习问题和模型架构，没有哪个优化器是最优的。甚至，比较不同优化器也是个困难的任务。

2. 我们推荐以流行的优化器，特别是当开始一个新的工程。

   a. 理想的，对相同类型的问题选择最流行的优化器

3. 应对所选择的优化器的所有超参数给予重视。

   a. 具有更多超参数的优化器可能需要更多的调节努力来寻找最佳设置
   
   b. 这特别的相对在工程开始阶段，当我们寻找其他多种超参数的最佳值（如，架构超参数）而对优化器超参数为麻烦的参数
   
   c. 也许在工程开始阶段，使用较简单的优化器（如，固定动量的SGD或固定参数的Adam）更合适，在以后再转向更一般的优化器

4. 我们喜欢公认的优化器包括（但不局限于）

   a. 动量SGD（我们喜欢Nesterov变种）

   b. Adam和NAdam，比动量SGD更一般。注意，Adam有4个可调节超参数。

### 选择batch size

**总结**：batch size控制着训练速度，不应被用于直接调节验证集的性能。通常，理想的batch size是现有硬件所支持的最大的batch size。

	1. batch size是决定训练时间和计算资源消耗的关键参数
	1. 增加batch size通常可以降低训练时间。这































